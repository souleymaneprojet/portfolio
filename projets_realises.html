<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Projets Réalisés</title>
  <link rel="stylesheet" href="style.css">

  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VDNMV1EVS3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VDNMV1EVS3');
</script>
  
</head>
<body>
  <header>
    <h1>Projets Réalisés</h1>
    <nav>
       <a href="index.html">Accueil</a>
      <a href="competences.html">Compétences</a>
      <a href="experiences.html">Expériences</a>
      <a href="formation.html">Formation</a>
      <a href="projets_realises.html">Projets</a>
      <a href="certifications.html">Certifications</a>
       <a href="recommandations.html">Recommandations</a> |
      <a href="contact.html">Contact</a>
    </nav>
  </header>
  <main>

 <h1>Projets Réalisés en Entreprise et en Milieu Académique</h1>
    <h2>Les applications de Souleymane daffe</h2>

  <h1><stron><h2>Si vous cliquez sur l’une des applications et que l’écran « Zzz » apparaît, cliquez sur le bouton bleu puis patientez 30–60 s.</h2>
.</stron></h1>

  <p>
   Application de détection de l'obésité :  
  <a href="https://obeise-iqjvjjq9pgsqpqvi54w8ge.streamlit.app/" target="_blank" rel="noopener noreferrer">
    Obesity Detection 
  </a>
</p>
   <p>
   Application de prediction d'énergie :  
  <a href="https://energie-bnjhufarwbue6nqjhmrucz.streamlit.app/" target="_blank" rel="noopener noreferrer">
    prediction d'énergie (Streamlit)
  </a>
</p>
    
<p>
  Analyse des Ventes en Ligne :  
  <a href="https://finance-2tgwb32r7a9dqptfqb74ux.streamlit.app/" target="_blank" rel="noopener noreferrer">
    Prédiction des Ventes
  </a>
</p> 



<p>
   Prédicteur de rentabilité de commande :  
  <a href="https://commandeappuit-gsescfaszplm7r4pdzvifc.streamlit.app/" target="_blank" rel="noopener noreferrer">
    Prédiction de rentabilité de commande
  </a>
</p>

    <p>
  Application de détection de Fraude Bancaire :  
  <a href="https://fraudecarte-fkgczddrclvhmfsjds5tw6.streamlit.app/ " target="_blank" rel="noopener noreferrer">
    Détection de Fraude
  </a>
</p>

     <p>
   Prédiction manuelle de prix d’une voiture d’occasion :  
  <a href="https://predireprixvoiture-eiung6bdbftprvgnwshbbj.streamlit.app/" target="_blank" rel="noopener noreferrer">
    Prédiction de prix d’une voiture d’occasion
  </a>
</p>
    


 
   <p>
   IA de Classement Automatique des Candidats :  
  <a href="https://analyselescv-aspf8qcwm9prnsolb8xju9.streamlit.app/" target="_blank" rel="noopener noreferrer">
    Analyse les CV
  </a>
</p>

       <p>
   Analyse du Désabonnement des Clients dans Télécom :  
  <a href="https://telecommunication-bmzvabxg4fj7eb3bs98cof.streamlit.app/" target="_blank" rel="noopener noreferrer">
     Désabonnement des Clients 
</p>


 <section class="project">
  <h2>Prévision de la consommation d’énergie électrique</h2>
  <p><strong>Contexte :</strong> Analyse de séries temporelles visant à prévoir la consommation électrique et à détecter des anomalies ou des effets saisonniers. L’objectif principal est :</p>
  <ul>
    <li>de comprendre l’évolution de la consommation d’électricité au fil du temps,</li>
    <li>de détecter des comportements saisonniers (par exemple, une hausse en hiver),</li>
    <li>d’identifier des anomalies ou des pics inhabituels,</li>
    <li>et surtout, de prévoir la consommation future pour mieux planifier la production.</li>
  </ul>

  <p><strong>Tâches effectuées :</strong> 
  J’ai analysé une série temporelle de 2012 à 2021 afin de :
  <ul>
    <li>mettre en évidence les variables les plus influentes sur la consommation,</li>
    <li>détecter une éventuelle saisonnalité,</li>
    <li>préparer les données pour appliquer un modèle ARIMA ou SARIMA.</li>
  </ul>
  J’ai utilisé la méthode SARIMA pour réaliser les prédictions, en automatisant la sélection des meilleurs paramètres grâce à une fonction Python dédiée. Le modèle final a été évalué à l’aide de visualisations et d’indicateurs statistiques.
  </p>

  <p><strong>Outils :</strong> Python, ARIMA, SARIMA, SARIMAX, matplotlib, seaborn, statsmodels</p>
</section>

 <section class="project">
  <h2>Prédiction du Prix des Appartements avec le Machine Learning</h2>

  <p><strong>Contexte :</strong><br>
    Le marché immobilier est influencé par de nombreux facteurs tels que la localisation, la surface habitable, le nombre de pièces ou encore l'année de construction.  
    Dans ce projet, j’ai cherché à créer un modèle prédictif fiable du prix de vente d’un appartement en me basant sur ses caractéristiques principales.  
    L’objectif est d’aider les agences, les particuliers ou les investisseurs à estimer plus justement la valeur d’un bien.
  </p>

  <p><strong>Tâches effectuées :</strong></p>
  <ol>
    <li><strong>Collecte de données</strong><br>
      J’ai utilisé un dataset public provenant de la plateforme Kaggle. Il contient des informations détaillées sur des appartements : surface, nombre de pièces, localisation, année de construction, ainsi que le prix de vente.  
      J’aurais également pu recourir au web scraping (sur des sites comme LeBonCoin ou SeLoger), mais j’ai préféré une base déjà structurée pour me concentrer sur l’analyse et la modélisation.
    </li>

    <li><strong>Exploration et Prétraitement des Données</strong>
      <ul>
        <li><strong>Analyse exploratoire (EDA)</strong> : j’ai utilisé des statistiques descriptives et des visualisations (histogrammes, boxplots, heatmap de corrélation) pour mieux comprendre la distribution des variables et identifier les relations avec le prix.</li>
        <li><strong>Nettoyage des données</strong> : j’ai supprimé ou imputé les valeurs manquantes, éliminé les doublons et vérifié la cohérence des données.</li>
        <li><strong>Transformation des variables</strong> :
          <ul>
            <li>J’ai encodé les variables catégorielles (comme la localisation) avec <code>OneHotEncoder</code>.</li>
            <li>J’ai standardisé et normalisé les variables numériques pour les modèles sensibles à l’échelle (comme KNN).</li>
            <li>J’ai également créé de nouvelles variables comme le prix au m² ou l’âge du bâtiment.</li>
          </ul>
        </li>
      </ul>
    </li>

    <li><strong>Modélisation avec le Machine Learning</strong>
      <ul>
        <li>J’ai divisé le dataset en deux parties : 80 % pour l'entraînement et 20 % pour les tests.</li>
        <li>J’ai entraîné plusieurs modèles de régression afin de comparer leurs performances :
          <ul>
            <li>Régression Linéaire</li>
            <li>K-Nearest Neighbors (KNN)</li>
            <li>Random Forest Regressor</li>
            <li>Gradient Boosting (XGBoost et LightGBM)</li>
          </ul>
        </li>
        <li>Pour les évaluer, j’ai utilisé les métriques suivantes :
          <ul>
            <li>MAE (Mean Absolute Error)</li>
            <li>RMSE (Root Mean Squared Error)</li>
            <li>R² (coefficient de détermination)</li>
            <li>Et des graphes de prédiction vs valeurs réelles</li>
          </ul>
        </li>
      </ul>
    </li>

    <li><strong>Optimisation des modèles</strong>
      <ul>
        <li>J’ai optimisé les hyperparamètres grâce à <code>GridSearchCV</code> et <code>RandomizedSearchCV</code>.</li>
        <li>J’ai effectué une sélection des variables les plus importantes pour simplifier les modèles et éviter l’overfitting.</li>
      </ul>
    </li>

    <li><strong>Visualisation et interprétation</strong>
      <ul>
        <li>J’ai affiché une heatmap de corrélation pour visualiser les dépendances entre variables.</li>
        <li>J’ai analysé l’importance des variables dans les modèles basés sur les arbres (Random Forest, XGBoost).</li>
        <li>J’ai utilisé des courbes d’apprentissage pour détecter d’éventuels problèmes de sous-apprentissage ou sur-apprentissage.</li>
      </ul>
    </li>
  </ol>

  <p><strong>Outils & Bibliothèques utilisés :</strong></p>
  <ul>
    <li><strong>Langage :</strong> Python 3.x</li>
    <li><strong>Analyse de données :</strong>
      <ul>
        <li><code>pandas</code></li>
        <li><code>numpy</code></li>
      </ul>
    </li>
    <li><strong>Visualisation :</strong>
      <ul>
        <li><code>matplotlib</code></li>
        <li><code>seaborn</code></li>
        <li><code>plotly</code> (pour les visualisations interactives)</li>
      </ul>
    </li>
    <li><strong>Machine Learning :</strong>
      <ul>
        <li><code>scikit-learn</code> : régression, KNN, Random Forest, métriques d’évaluation</li>
        <li><code>xgboost</code> et <code>lightgbm</code> : Gradient Boosting</li>
      </ul>
    </li>
    <li><strong>Optimisation :</strong>
      <ul>
        <li><code>sklearn.model_selection</code> : <code>train_test_split</code>, <code>GridSearchCV</code>, <code>RandomizedSearchCV</code></li>
      </ul>
    </li>
  </ul>
</section>


  
    <section class="project">
    <h2>Calcul de la performance commerciale, analyse de données et visualisation avec Power BI</h2>
    <p><strong>Contexte :</strong> Création d’un tableau de bord Power BI mettant en avant :
      <ul>
        <li>Les produits les plus performants</li>
        <li>Les pays ou clients les plus rentables</li>
        <li>Des indicateurs de succès clairs</li>
        <li>Des axes d'amélioration potentiels</li>
      </ul>
    </p>
    <p><strong>Tâches effectuées :</strong> Pour évaluer la performance, j’ai d’abord mis en place une table de dates. Ensuite, j’ai réalisé plusieurs opérations : création d’une colonne pour le montant total des ventes, définition d’une mesure pour le total des ventes, puis calcul des ventes des années précédentes afin de mesurer les performances des produits. J’ai également créé une mesure nommée "Ventes précédentes", permettant cette comparaison, ainsi qu’une mesure "Évolution des ventes" pour visualiser les tendances par produit.</p>
    <p><strong>Outils :</strong> Power BI, DAX, Power Query, mesures, colonnes calculées, Excel</p>
  </section>

    <section class="project">
  <h2>Scraping de données web et traitement analytique</h2>
  
  <p><strong>Contexte :</strong> 
    Ce projet s’inscrit dans une démarche d’apprentissage du traitement et de l’analyse de données.  
    Il consistait à extraire des informations librement accessibles sur Internet, notamment depuis Wikipedia.  
    Le sujet portait sur une liste de pays avec des indicateurs comme la population et la superficie, disponibles sous forme de tableaux HTML.  
    L’objectif était de transformer ces données brutes en un jeu de données structuré, exploitable pour l’analyse et la visualisation.  
    Ce projet a permis de développer des compétences en web scraping, manipulation de données et visualisation avec Python.
  </p>
  
  <p><strong>Tâches effectuées :</strong> 
    J’ai identifié une page Wikipedia contenant des données sur les pays (population, superficie, etc.).  
    À l’aide de l’inspecteur HTML, j’ai localisé les balises <code>&lt;table&gt;</code> pertinentes, puis utilisé la bibliothèque <code>requests</code> pour récupérer le contenu.  
    Ensuite, j’ai extrait les données ciblées à l’aide de <code>BeautifulSoup</code>, nettoyé les formats, supprimé les balises HTML inutiles, et converti les types de données.  
    Les informations ont été structurées dans un <code>DataFrame</code> avec <code>pandas</code>, analysées selon différents critères, puis visualisées via des graphiques.  
    Enfin, le jeu de données a été exporté au format CSV.
  </p>

  <p><strong>Outils :</strong> Python, requests, BeautifulSoup, pandas, matplotlib, seaborn (utilisés dans un Jupyter Notebook).</p>
</section>

     <section class="project">
  <h2>Extraction de données depuis des documents PDF</h2>

  <p><strong>Contexte :</strong>  
  L’objectif était de collecter des données à partir de documents PDF (avec ou sans couche texte), d’extraire automatiquement des informations clés et de les structurer dans un fichier CSV contenant les colonnes suivantes :  
  <code>"enterprise_name", "raison_d_etre", "objectifs_sociaux", "objectifs_environnementaux", "mission_nature"</code>.  
  Le projet s’appuie sur des techniques d’OCR, de traitement de texte et d’intelligence artificielle (modèle OpenAI GPT).</p>

  <p><strong>Tâches effectuées :</strong>  
    - Identification des PDF contenant une couche texte ou non à l’aide de <code>pdfplumber</code> ;<br>
    - Utilisation de <code>pdfplumber</code> pour les PDF textuels, et de <code>pytesseract</code> pour l’OCR des PDF scannés ;<br>
    - Développement d’une fonction de découpage du texte en fragments pour respecter les limites de tokens des modèles IA ;<br>
    - Création d’un système de requêtes structurées vers le modèle GPT pour extraire des informations spécifiques (nom d’entreprise, raison d’être, missions RSE, etc.) ;<br>
    - Implémentation d’une logique d’agrégation des résultats issus de plusieurs pages (chunks) ;<br>
    - Traitement automatisé d’un dossier complet de PDF, avec sauvegarde des résultats dans un fichier CSV ;<br>
    - Gestion des erreurs, vérification des réponses de l’API, gestion des cas d’échec avec fallback JSON sécurisé.</p>

  <p><strong>Outils :</strong> Python, pdfplumber, pytesseract, OpenAI API, CSV, JSON</p>
</section>

  <section class="project">
  <h2>Analyse des Risques d'Obésité dans les Populations du Mexique, du Pérou et de la Colombie : Application de l'Intelligence Artificielle aux Habitudes Alimentaires et à la Condition Physique</h2>
  
  <p><strong>Contexte :</strong><br>
    L’obésité est un enjeu de santé publique croissant en Amérique latine, notamment au Mexique, au Pérou et en Colombie. Ce projet vise à analyser les facteurs de risque (alimentation, activité physique, habitudes de vie) liés à l’obésité dans ces pays, en utilisant des techniques d’intelligence artificielle.<br>
    Les données collectées concernent 2111 individus avec 18 variables (9 qualitatives, 8 quantitatives et 1 calculée : l’IMC). La variable cible est <em>"NObeyesdad"</em>, qui catégorise 7 niveaux d’obésité.<br><br>
    <strong>Objectifs :</strong>
    <ul>
      <li>Prédire les niveaux d’obésité à l’aide de modèles de machine learning.</li>
      <li>Identifier les facteurs influents pour fournir des recommandations de santé publique adaptées à chaque pays.</li>
    </ul>
  </p>
  
  <p><strong>Tâches effectuées :</strong></p>
  <ol>
    <li><strong>Compréhension & Préparation des Données</strong>
      <ul>
        <li>Analyse des variables qualitatives et quantitatives</li>
        <li>Ajout de l’IMC (poids / taille²)</li>
        <li>Détection des valeurs manquantes et aberrantes</li>
        <li>Recodage des variables numériques décimales (FCVC, NCP, CH2O, FAF, TUE)</li>
        <li>Encodage des variables catégorielles avec <code>LabelEncoder()</code></li>
      </ul>
    </li>
    <li><strong>Nettoyage et Structuration</strong>
      <ul>
        <li>Suppression de variables non pertinentes : <em>TUE, CALC, CH2O, SMOKE, Gender</em></li>
        <li>Création de plusieurs DataFrames : <em>obese, obesecluster, obesecluster_array, attributs_data, target_data</em></li>
      </ul>
    </li>
    <li><strong>Analyse exploratoire</strong>
      <ul>
        <li>Matrice de corrélation pour étudier les relations entre les variables</li>
        <li>Identification des variables influentes dans la prédiction de l’obésité</li>
      </ul>
    </li>
    <li><strong>Apprentissage non supervisé</strong>
      <ul>
        <li>Clustering K-Means avec 7 puis 5 clusters</li>
        <li>Réduction de dimension (PCA) pour visualisation 2D</li>
      </ul>
    </li>
    <li><strong>Apprentissage supervisé</strong>
      <p>Modèles utilisés :</p>
      <ul>
        <li>DecisionTreeClassifier (Accuracy ≈ 96%)</li>
        <li>DecisionTreeRegressor (≈ 96.5%)</li>
        <li>Gaussian Naive Bayes (≈ 83.7%)</li>
        <li>Régression Logistique (≈ 96.7%)</li>
        <li>Support Vector Machine (SVM) (≈ 96.06%)</li>
        <li>K-Nearest Neighbors (KNN) (≈ 92.27%)</li>
      </ul>
      <p>Évaluation des modèles :</p>
      <ul>
        <li>Accuracy, Précision, Rappel, F1-Score</li>
        <li>Matrices de confusion, courbes ROC</li>
        <li>Erreurs : absolue, quadratique (EQ), EQM</li>
      </ul>
    </li>
    <li><strong>Sélection du modèle optimal</strong>
      <ul>
        <li>Modèles les plus performants : Régression logistique, DecisionTreeRegressor, SVM</li>
      </ul>
    </li>
    <li><strong>Règles de classification interprétables</strong>
      <ul>
        <li>Création de règles simples basées sur l’IMC, la taille, le poids, les antécédents familiaux, etc.</li>
      </ul>
    </li>
    <li><strong>Évaluation financière (étude complémentaire)</strong>
      <ul>
        <li>Analyse coût-bénéfice des politiques publiques de prévention</li>
        <li>Évaluation du retour sur investissement (ROI), bénéfices économiques et sociaux</li>
      </ul>
    </li>
  </ol>

  <p><strong>Outils :</strong></p>
  <ul>
    <li><strong>Langage :</strong> Python</li>
    <li><strong>Bibliothèques principales :</strong>
      <table border="1" cellpadding="5" cellspacing="0">
        <thead>
          <tr>
            <th>Bibliothèque</th>
            <th>Utilité principale</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>pandas</code></td>
            <td>Manipulation et structuration des données</td>
          </tr>
          <tr>
            <td><code>numpy</code></td>
            <td>Calculs numériques</td>
          </tr>
          <tr>
            <td><code>matplotlib</code>, <code>seaborn</code></td>
            <td>Visualisation de données et des courbes de ROC</td>
          </tr>
          <tr>
            <td><code>scikit-learn (sklearn)</code></td>
            <td>Modélisation : DecisionTree, SVM, kNN, LogisticRegression, etc.</td>
          </tr>
          <tr>
            <td><code>LabelEncoder</code></td>
            <td>Encodage des variables catégorielles</td>
          </tr>
          <tr>
            <td><code>KMeans</code>, <code>PCA</code></td>
            <td>Clustering et réduction de dimensions</td>
          </tr>
        </tbody>
      </table>
    </li>
  </ul>
</section>

     <section class="project">
  <h2>Détection de fake news via Deep Learning</h2>
  <p><strong>Contexte :</strong> Lutte contre la désinformation diffusée sur les réseaux sociaux, notamment concernant la santé des dirigeants.</p>

  <p><strong>Tâches effectuées :</strong> Collecte de données (Facebook, Twitter, etc.), traitement en CSV, NLP, entraînement de modèles pour la détection automatique de fausses nouvelles.</p>

  <p>Pour ce projet, j’ai choisi de m’appuyer sur l’intelligence artificielle (IA), qui offre une solution prometteuse pour automatiser la détection des fake news. En utilisant des techniques avancées de traitement du langage naturel (NLP), j’ai récolté des données à partir de diverses sources telles que Facebook, Google, Twitter, la presse et YouTube.</p>

  <p>Par exemple, j’ai récupéré toutes les vidéos évoquant les fausses nouvelles sur la santé du président. Je me suis concentré sur les commentaires trouvés sur Google, j’ai collecté des tweets liés à ce sujet sur Twitter, rassemblé des articles de presse, et analysé les publications et commentaires sur Facebook.</p>

  <p>J’ai ensuite structuré mes données dans un fichier CSV comportant quatre colonnes :
    <ul>
      <li><strong>sources :</strong> origine des données (site web ou plateforme sociale),</li>
      <li><strong>commentaire :</strong> texte de la nouvelle ou du commentaire,</li>
      <li><strong>malade :</strong> identification du sujet (ici, la santé du président),</li>
      <li><strong>auteur :</strong> nom ou identifiant de l’auteur du contenu.</li>
    </ul>
  </p>

  <p>Après avoir préparé ces fichiers CSV, j’ai utilisé la bibliothèque Pandas en Python pour les concaténer en un seul fichier. Grâce à la fonction <code>concat()</code>, j’ai pu fusionner toutes les données en un fichier consolidé prêt à l’analyse.</p>

  <p>Ce fichier unique contient l’ensemble des données structurées. La colonne "sources" permet de retracer l’origine de chaque entrée, "commentaire" fournit le contenu textuel, "malade" permet d’identifier les fausses nouvelles liées à la santé, et "auteur" indique l’origine du contenu.</p>

  <p>Ce processus de collecte et de structuration des données est essentiel pour entraîner un système d’IA performant. Il facilite l’analyse des tendances, la détection des motifs récurrents dans les fausses nouvelles, et l’identification de sources peu fiables.</p>

  <p>En résumé, j’ai pris en charge l’intégralité de la collecte et de l’organisation des données, créant ainsi une base robuste pour mon projet de détection de fake news.</p>

  <p><strong>Outils :</strong> Python, Pandas, NumPy, Scikit-learn, TensorFlow, Matplotlib, Seaborn, CountVectorizer, TfidfVectorizer, Stopwords, Logistic Regression, Random Forest, SVM, <code>accuracy_score</code>, <code>f1_score</code>, <code>confusion_matrix</code>.</p>

</section>




  <section class="project">
  <h2>Assistant IA – Jeux Olympiques 2024</h2>

  <p><strong>Contexte :</strong>  
  Développement d’un chatbot prédictif basé sur l’IA pour répondre à toutes les questions liées aux Jeux Olympiques de Paris 2024, notamment les classements, les performances des athlètes, et les prévisions de médailles.</p>

  <p><strong>Tâches effectuées :</strong>  
  J’ai conçu et testé plusieurs prompts spécialisés afin d’adapter les réponses du chatbot selon le contexte :  
  <ul>
    <li><code>JO_PROMPT</code> : Fournit les informations officielles sur le classement des JO 2024.</li>
    <li><code>JO_PROMPT2</code> : Présente les performances remarquables et les médaillés marquants des JO.</li>
    <li><code>JO_PROMPT3</code> : Prompt destiné à un assistant expert en sport. Il analyse les chances de médailles d’un pays ou d’un athlète en se basant sur les résultats précédents, les classements mondiaux, et les tendances observées, sans jamais donner de certitudes.</li>
  </ul>
  Ce système de prompts permet une réponse contextuelle, fiable et nuancée selon le type de requête utilisateur.</p>

  <p><strong>Outils :</strong> Python, OpenAI API, Streamlit</p>
</section>

  <section class="project">
  <h2>Développement d'une application carbone</h2>
  <p>
    <strong>Contexte :</strong> Développement d’un outil innovant dans Revit pour estimer l’empreinte carbone des projets architecturaux dès les phases d’études préliminaires et d’avant-projet.  
    Cet outil permet de calculer une estimation globale du bilan carbone à partir des données disponibles dans le modèle Revit.  
    Il aide les décideurs à comprendre rapidement l’impact environnemental potentiel de leurs constructions et à prendre des décisions plus durables.
  </p>

  <p>
    L’outil a été conçu pour être facilement intégrable et simple d’utilisation. Il permet également de générer des rapports automatisés ainsi que des vues 3D colorées pour visualiser l’impact carbone de chaque composant du bâtiment.  
    j'ai  joué un rôle clé dans son développement, notamment dans la programmation en Python et l’intégration technique avec Revit.  
    Grâce à ma contribution, l’outil sera déployé chez AREP à l’été 2025.
  </p>

  <p><strong>Tâches effectuées :</strong> Développement d’une application sur Revit (Python/C#), génération automatique du calcul d’empreinte carbone, visualisation interactive avec Dash & Power BI, assistance à la prise de décision écologique.</p>

  <p><strong>Outils :</strong> Python, Revit, C#, Dash, Power BI, Git, GitHub.</p>

</section>



  </main>
  <footer>
    <p>Souleymane Daffe  DATA SCIENTIS/ANALYST/DEV IA</p>
  </footer>
</body>
</html>
